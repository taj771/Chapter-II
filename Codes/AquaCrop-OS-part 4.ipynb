{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-28T15:45:28.662813Z",
     "start_time": "2025-09-28T15:45:03.885014Z"
    }
   },
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "import psutil\n",
    "print(psutil.virtual_memory())\n",
    "!pip install git+https://github.com/aquacropos/aquacrop\n",
    "import os\n",
    "os.environ['DEVELOPMENT'] = 'True'"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=17179869184, available=4398809088, percent=74.4, used=6184042496, free=53903360, active=4365008896, inactive=4323868672, wired=1819033600)\n",
      "Collecting git+https://github.com/aquacropos/aquacrop\r\n",
      "  Cloning https://github.com/aquacropos/aquacrop to /private/var/folders/fp/rmn8kxk51dl98wnpbyxmh1500000gn/T/pip-req-build-za56ub4o\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/aquacropos/aquacrop /private/var/folders/fp/rmn8kxk51dl98wnpbyxmh1500000gn/T/pip-req-build-za56ub4o\r\n",
      "  Resolved https://github.com/aquacropos/aquacrop to commit f122773a63512bea24e163b3d87eb112fd49336b\r\n",
      "  Installing build dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Installing backend dependencies ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aquacrop==3.0.11) (1.22.4)\r\n",
      "Requirement already satisfied: pandas>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aquacrop==3.0.11) (2.2.3)\r\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aquacrop==3.0.11) (4.66.5)\r\n",
      "Requirement already satisfied: matplotlib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aquacrop==3.0.11) (3.9.2)\r\n",
      "Requirement already satisfied: seaborn>=0.13.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aquacrop==3.0.11) (0.13.2)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=1.2.0->aquacrop==3.0.11) (1.3.0)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=1.2.0->aquacrop==3.0.11) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=1.2.0->aquacrop==3.0.11) (4.54.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=1.2.0->aquacrop==3.0.11) (1.4.7)\r\n",
      "Collecting numpy>=1.22.0 (from aquacrop==3.0.11)\r\n",
      "  Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=1.2.0->aquacrop==3.0.11) (24.1)\r\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=1.2.0->aquacrop==3.0.11) (11.0.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=1.2.0->aquacrop==3.0.11) (3.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from matplotlib>=1.2.0->aquacrop==3.0.11) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=2.0.0->aquacrop==3.0.11) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas>=2.0.0->aquacrop==3.0.11) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=1.2.0->aquacrop==3.0.11) (1.16.0)\r\n",
      "Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\r\n",
      "Installing collected packages: numpy\r\n",
      "  Attempting uninstall: numpy\r\n",
      "    Found existing installation: numpy 1.22.4\r\n",
      "    Uninstalling numpy-1.22.4:\r\n",
      "      Successfully uninstalled numpy-1.22.4\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "numba 0.61.0 requires numpy<2.2,>=1.24, but you have numpy 2.2.6 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed numpy-2.2.6\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m25.0.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.2\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T15:45:59.474722Z",
     "start_time": "2025-09-28T15:45:54.192295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the downloaded Daymet data\n",
    "climate_data = pd.read_csv('/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/ClimateData/weather_data_Aqua.csv', on_bad_lines='skip')\n",
    "\n",
    "# Check if 'Date' column exists\n",
    "if 'Date' in climate_data.columns:\n",
    "    # Convert 'Date' column to datetime\n",
    "    climate_data['Date'] = pd.to_datetime(climate_data['Date'], errors='coerce')\n",
    "\n",
    "    # Remove rows with missing Date values\n",
    "    climate_data = climate_data.dropna(subset=['Date'])\n",
    "\n",
    "    # Extract Day, Month, and Year from the Date column\n",
    "    climate_data['Day'] = climate_data['Date'].dt.day\n",
    "    climate_data['Month'] = climate_data['Date'].dt.month\n",
    "    climate_data['Year'] = climate_data['Date'].dt.year\n",
    "    e_a = climate_data['e_a']  # actual vapor pressure (kpa)\n",
    "\n",
    "    # Calculate necessary climate variables\n",
    "    T_max = climate_data['MaxTemp']  # Maximum temperature (°C)\n",
    "    T_min = climate_data['MinTemp']  # Minimum temperature (°C)\n",
    "    precipitation = climate_data['Precipitation']  # Precipitation (mm)\n",
    "    solar_radiation = climate_data['R_n']  # Solar radiation (MJ/m²/day)\n",
    "\n",
    "    # Calculate mean temperature\n",
    "    T_mean = (T_max + T_min) / 2\n",
    "\n",
    "    # Calculate saturation vapor pressure (e_s) in kPa\n",
    "    e_s = 0.6108 * np.exp((17.27 * T_mean) / (T_mean + 237.3))\n",
    "\n",
    "    # Assuming relative humidity (RH) is available or use an average value\n",
    "    #RH = 62  # Example average RH of 50%\n",
    "    #e_a = (RH / 100) * e_s  # Actual vapor pressure in kPa\n",
    "\n",
    "    # Define other constants\n",
    "    R_n = solar_radiation  # Net radiation (MJ/m²/day)\n",
    "    G = 0  # Soil heat flux density (MJ/m²/day), often negligible\n",
    "    gamma = 0.066  # Psychrometric constant (kPa/°C)\n",
    "    u = 2  # Average wind speed in m/s (if available, else use an estimated value)\n",
    "\n",
    "    # Calculate slope of the saturation vapor pressure curve (Δ)\n",
    "    delta = (4098 * e_s) / ((T_mean + 237.3) ** 2)\n",
    "\n",
    "    # Calculate reference ET (ET₀) using the Penman-Monteith equation\n",
    "    ReferenceET = (0.408 * delta * (R_n - G) + gamma * (900 / (T_mean + 273)) * u * (e_s - e_a)) / (delta + gamma * (1 + 0.34 * u))\n",
    "\n",
    "    # Add ET₀ to the DataFrame\n",
    "    climate_data['ReferenceET'] = ReferenceET\n",
    "\n",
    "    # Reorder the columns to bring Day, Month, Year first\n",
    "    climate_data = climate_data[['Day', 'Month', 'Year'] + [col for col in climate_data.columns if col not in ['Day', 'Month', 'Year']]]\n",
    "\n",
    "    # Save the DataFrame to a text file (CSV format)\n",
    "    climate_data.to_csv('daymet_data_with_et0.csv', index=False)  # Save as a tab-separated text file\n",
    "\n",
    "    print(\"Reference ET₀ calculated and saved to 'daymet_data_with_et0.csv'.\")\n",
    "else:\n",
    "    print(\"The 'Date' column does not exist in the dataset.\")"
   ],
   "id": "f8d76d250910f186",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference ET₀ calculated and saved to 'daymet_data_with_et0.csv'.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T15:46:24.656131Z",
     "start_time": "2025-09-28T15:46:21.480595Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"daymet_data_with_et0.csv\")\n",
    "\n",
    "# Get a list of unique sites\n",
    "unique_sites = df['site'].unique()\n",
    "\n",
    "# Specify the directories where the weather files will be saved\n",
    "weather_dir = \"./ClimateData\"\n",
    "os.makedirs(weather_dir, exist_ok=True)\n",
    "\n",
    "# Define the columns to select and rename\n",
    "columns_to_select = ['Day', 'Month', 'Year', 'MinTemp', 'MaxTemp', 'Precipitation', 'ReferenceET']\n",
    "columns_to_rename = {\n",
    "    'MinTemp': 'Tmin(c)',\n",
    "    'MaxTemp': 'Tmax(c)',\n",
    "    'Precipitation': 'Prcp(mm)',\n",
    "    'ReferenceET': 'Et0(mm)'\n",
    "}\n",
    "\n",
    "# Loop through each unique site and save its weather data to a text file\n",
    "for site in unique_sites:\n",
    "    df_filtered = df[df['site'] == site][columns_to_select].rename(columns=columns_to_rename)\n",
    "\n",
    "    # Create a filename for the current site's weather data\n",
    "    weather_filename = os.path.join(weather_dir, f\"site_{site}.txt\")\n",
    "\n",
    "    # Save the filtered data to the weather file\n",
    "    df_filtered.to_csv(weather_filename, sep='\\t', index=False)\n",
    "    print(f\"Weather data saved for site {site}.\")\n"
   ],
   "id": "bfe33fa51da3aea4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data saved for site 1.\n",
      "Weather data saved for site 2.\n",
      "Weather data saved for site 3.\n",
      "Weather data saved for site 4.\n",
      "Weather data saved for site 5.\n",
      "Weather data saved for site 6.\n",
      "Weather data saved for site 7.\n",
      "Weather data saved for site 8.\n",
      "Weather data saved for site 9.\n",
      "Weather data saved for site 10.\n",
      "Weather data saved for site 11.\n",
      "Weather data saved for site 12.\n",
      "Weather data saved for site 13.\n",
      "Weather data saved for site 14.\n",
      "Weather data saved for site 15.\n",
      "Weather data saved for site 16.\n",
      "Weather data saved for site 17.\n",
      "Weather data saved for site 18.\n",
      "Weather data saved for site 19.\n",
      "Weather data saved for site 20.\n",
      "Weather data saved for site 21.\n",
      "Weather data saved for site 22.\n",
      "Weather data saved for site 23.\n",
      "Weather data saved for site 24.\n",
      "Weather data saved for site 25.\n",
      "Weather data saved for site 26.\n",
      "Weather data saved for site 27.\n",
      "Weather data saved for site 28.\n",
      "Weather data saved for site 29.\n",
      "Weather data saved for site 30.\n",
      "Weather data saved for site 31.\n",
      "Weather data saved for site 32.\n",
      "Weather data saved for site 33.\n",
      "Weather data saved for site 34.\n",
      "Weather data saved for site 35.\n",
      "Weather data saved for site 36.\n",
      "Weather data saved for site 37.\n",
      "Weather data saved for site 38.\n",
      "Weather data saved for site 39.\n",
      "Weather data saved for site 40.\n",
      "Weather data saved for site 41.\n",
      "Weather data saved for site 42.\n",
      "Weather data saved for site 43.\n",
      "Weather data saved for site 44.\n",
      "Weather data saved for site 45.\n",
      "Weather data saved for site 46.\n",
      "Weather data saved for site 47.\n",
      "Weather data saved for site 48.\n",
      "Weather data saved for site 49.\n",
      "Weather data saved for site 50.\n",
      "Weather data saved for site 51.\n",
      "Weather data saved for site 52.\n",
      "Weather data saved for site 53.\n",
      "Weather data saved for site 54.\n",
      "Weather data saved for site 55.\n",
      "Weather data saved for site 56.\n",
      "Weather data saved for site 57.\n",
      "Weather data saved for site 58.\n",
      "Weather data saved for site 59.\n",
      "Weather data saved for site 60.\n",
      "Weather data saved for site 61.\n",
      "Weather data saved for site 62.\n",
      "Weather data saved for site 63.\n",
      "Weather data saved for site 64.\n",
      "Weather data saved for site 65.\n",
      "Weather data saved for site 66.\n",
      "Weather data saved for site 67.\n",
      "Weather data saved for site 68.\n",
      "Weather data saved for site 69.\n",
      "Weather data saved for site 70.\n",
      "Weather data saved for site 71.\n",
      "Weather data saved for site 72.\n",
      "Weather data saved for site 73.\n",
      "Weather data saved for site 74.\n",
      "Weather data saved for site 75.\n",
      "Weather data saved for site 76.\n",
      "Weather data saved for site 77.\n",
      "Weather data saved for site 78.\n",
      "Weather data saved for site 79.\n",
      "Weather data saved for site 80.\n",
      "Weather data saved for site 81.\n",
      "Weather data saved for site 82.\n",
      "Weather data saved for site 83.\n",
      "Weather data saved for site 84.\n",
      "Weather data saved for site 85.\n",
      "Weather data saved for site 86.\n",
      "Weather data saved for site 87.\n",
      "Weather data saved for site 88.\n",
      "Weather data saved for site 89.\n",
      "Weather data saved for site 90.\n",
      "Weather data saved for site 91.\n",
      "Weather data saved for site 92.\n",
      "Weather data saved for site 93.\n",
      "Weather data saved for site 94.\n",
      "Weather data saved for site 95.\n",
      "Weather data saved for site 96.\n",
      "Weather data saved for site 97.\n",
      "Weather data saved for site 98.\n",
      "Weather data saved for site 99.\n",
      "Weather data saved for site 100.\n",
      "Weather data saved for site 101.\n",
      "Weather data saved for site 102.\n",
      "Weather data saved for site 103.\n",
      "Weather data saved for site 104.\n",
      "Weather data saved for site 105.\n",
      "Weather data saved for site 106.\n",
      "Weather data saved for site 107.\n",
      "Weather data saved for site 108.\n",
      "Weather data saved for site 109.\n",
      "Weather data saved for site 110.\n",
      "Weather data saved for site 111.\n",
      "Weather data saved for site 112.\n",
      "Weather data saved for site 113.\n",
      "Weather data saved for site 114.\n",
      "Weather data saved for site 115.\n",
      "Weather data saved for site 116.\n",
      "Weather data saved for site 117.\n",
      "Weather data saved for site 118.\n",
      "Weather data saved for site 119.\n",
      "Weather data saved for site 120.\n",
      "Weather data saved for site 121.\n",
      "Weather data saved for site 122.\n",
      "Weather data saved for site 123.\n",
      "Weather data saved for site 124.\n",
      "Weather data saved for site 125.\n",
      "Weather data saved for site 126.\n",
      "Weather data saved for site 127.\n",
      "Weather data saved for site 128.\n",
      "Weather data saved for site 129.\n",
      "Weather data saved for site 130.\n",
      "Weather data saved for site 131.\n",
      "Weather data saved for site 132.\n",
      "Weather data saved for site 133.\n",
      "Weather data saved for site 134.\n",
      "Weather data saved for site 135.\n",
      "Weather data saved for site 136.\n",
      "Weather data saved for site 137.\n",
      "Weather data saved for site 138.\n",
      "Weather data saved for site 139.\n",
      "Weather data saved for site 140.\n",
      "Weather data saved for site 141.\n",
      "Weather data saved for site 142.\n",
      "Weather data saved for site 143.\n",
      "Weather data saved for site 144.\n",
      "Weather data saved for site 145.\n",
      "Weather data saved for site 146.\n",
      "Weather data saved for site 147.\n",
      "Weather data saved for site 148.\n",
      "Weather data saved for site 149.\n",
      "Weather data saved for site 150.\n",
      "Weather data saved for site 151.\n",
      "Weather data saved for site 152.\n",
      "Weather data saved for site 153.\n",
      "Weather data saved for site 154.\n",
      "Weather data saved for site 155.\n",
      "Weather data saved for site 156.\n",
      "Weather data saved for site 157.\n",
      "Weather data saved for site 158.\n",
      "Weather data saved for site 159.\n",
      "Weather data saved for site 160.\n",
      "Weather data saved for site 161.\n",
      "Weather data saved for site 162.\n",
      "Weather data saved for site 163.\n",
      "Weather data saved for site 164.\n",
      "Weather data saved for site 165.\n",
      "Weather data saved for site 166.\n",
      "Weather data saved for site 167.\n",
      "Weather data saved for site 168.\n",
      "Weather data saved for site 169.\n",
      "Weather data saved for site 170.\n",
      "Weather data saved for site 171.\n",
      "Weather data saved for site 172.\n",
      "Weather data saved for site 173.\n",
      "Weather data saved for site 174.\n",
      "Weather data saved for site 175.\n",
      "Weather data saved for site 176.\n",
      "Weather data saved for site 177.\n",
      "Weather data saved for site 178.\n",
      "Weather data saved for site 179.\n",
      "Weather data saved for site 180.\n",
      "Weather data saved for site 181.\n",
      "Weather data saved for site 182.\n",
      "Weather data saved for site 183.\n",
      "Weather data saved for site 184.\n",
      "Weather data saved for site 185.\n",
      "Weather data saved for site 186.\n",
      "Weather data saved for site 187.\n",
      "Weather data saved for site 188.\n",
      "Weather data saved for site 189.\n",
      "Weather data saved for site 190.\n",
      "Weather data saved for site 191.\n",
      "Weather data saved for site 192.\n",
      "Weather data saved for site 193.\n",
      "Weather data saved for site 194.\n",
      "Weather data saved for site 195.\n",
      "Weather data saved for site 196.\n",
      "Weather data saved for site 197.\n",
      "Weather data saved for site 198.\n",
      "Weather data saved for site 199.\n",
      "Weather data saved for site 200.\n",
      "Weather data saved for site 201.\n",
      "Weather data saved for site 202.\n",
      "Weather data saved for site 203.\n",
      "Weather data saved for site 204.\n",
      "Weather data saved for site 205.\n",
      "Weather data saved for site 206.\n",
      "Weather data saved for site 207.\n",
      "Weather data saved for site 208.\n",
      "Weather data saved for site 209.\n",
      "Weather data saved for site 210.\n",
      "Weather data saved for site 211.\n",
      "Weather data saved for site 212.\n",
      "Weather data saved for site 213.\n",
      "Weather data saved for site 214.\n",
      "Weather data saved for site 215.\n",
      "Weather data saved for site 216.\n",
      "Weather data saved for site 217.\n",
      "Weather data saved for site 218.\n",
      "Weather data saved for site 219.\n",
      "Weather data saved for site 220.\n",
      "Weather data saved for site 221.\n",
      "Weather data saved for site 222.\n",
      "Weather data saved for site 223.\n",
      "Weather data saved for site 224.\n",
      "Weather data saved for site 225.\n",
      "Weather data saved for site 226.\n",
      "Weather data saved for site 227.\n",
      "Weather data saved for site 228.\n",
      "Weather data saved for site 229.\n",
      "Weather data saved for site 230.\n",
      "Weather data saved for site 231.\n",
      "Weather data saved for site 232.\n",
      "Weather data saved for site 233.\n",
      "Weather data saved for site 234.\n",
      "Weather data saved for site 235.\n",
      "Weather data saved for site 236.\n",
      "Weather data saved for site 237.\n",
      "Weather data saved for site 238.\n",
      "Weather data saved for site 239.\n",
      "Weather data saved for site 240.\n",
      "Weather data saved for site 241.\n",
      "Weather data saved for site 242.\n",
      "Weather data saved for site 243.\n",
      "Weather data saved for site 244.\n",
      "Weather data saved for site 245.\n",
      "Weather data saved for site 246.\n",
      "Weather data saved for site 247.\n",
      "Weather data saved for site 248.\n",
      "Weather data saved for site 249.\n",
      "Weather data saved for site 250.\n",
      "Weather data saved for site 251.\n",
      "Weather data saved for site 252.\n",
      "Weather data saved for site 253.\n",
      "Weather data saved for site 254.\n",
      "Weather data saved for site 255.\n",
      "Weather data saved for site 256.\n",
      "Weather data saved for site 257.\n",
      "Weather data saved for site 258.\n",
      "Weather data saved for site 259.\n",
      "Weather data saved for site 260.\n",
      "Weather data saved for site 261.\n",
      "Weather data saved for site 262.\n",
      "Weather data saved for site 263.\n",
      "Weather data saved for site 264.\n",
      "Weather data saved for site 265.\n",
      "Weather data saved for site 266.\n",
      "Weather data saved for site 267.\n",
      "Weather data saved for site 268.\n",
      "Weather data saved for site 269.\n",
      "Weather data saved for site 270.\n",
      "Weather data saved for site 271.\n",
      "Weather data saved for site 272.\n",
      "Weather data saved for site 273.\n",
      "Weather data saved for site 274.\n",
      "Weather data saved for site 275.\n",
      "Weather data saved for site 276.\n",
      "Weather data saved for site 277.\n",
      "Weather data saved for site 278.\n",
      "Weather data saved for site 279.\n",
      "Weather data saved for site 280.\n",
      "Weather data saved for site 281.\n",
      "Weather data saved for site 282.\n",
      "Weather data saved for site 283.\n",
      "Weather data saved for site 284.\n",
      "Weather data saved for site 285.\n",
      "Weather data saved for site 286.\n",
      "Weather data saved for site 287.\n",
      "Weather data saved for site 288.\n",
      "Weather data saved for site 289.\n",
      "Weather data saved for site 290.\n",
      "Weather data saved for site 291.\n",
      "Weather data saved for site 292.\n",
      "Weather data saved for site 293.\n",
      "Weather data saved for site 294.\n",
      "Weather data saved for site 295.\n",
      "Weather data saved for site 296.\n",
      "Weather data saved for site 297.\n",
      "Weather data saved for site 298.\n",
      "Weather data saved for site 299.\n",
      "Weather data saved for site 300.\n",
      "Weather data saved for site 301.\n",
      "Weather data saved for site 302.\n",
      "Weather data saved for site 303.\n",
      "Weather data saved for site 304.\n",
      "Weather data saved for site 305.\n",
      "Weather data saved for site 306.\n",
      "Weather data saved for site 307.\n",
      "Weather data saved for site 308.\n",
      "Weather data saved for site 309.\n",
      "Weather data saved for site 310.\n",
      "Weather data saved for site 311.\n",
      "Weather data saved for site 312.\n",
      "Weather data saved for site 313.\n",
      "Weather data saved for site 314.\n",
      "Weather data saved for site 315.\n",
      "Weather data saved for site 316.\n",
      "Weather data saved for site 317.\n",
      "Weather data saved for site 318.\n",
      "Weather data saved for site 319.\n",
      "Weather data saved for site 320.\n",
      "Weather data saved for site 321.\n",
      "Weather data saved for site 322.\n",
      "Weather data saved for site 323.\n",
      "Weather data saved for site 324.\n",
      "Weather data saved for site 325.\n",
      "Weather data saved for site 326.\n",
      "Weather data saved for site 327.\n",
      "Weather data saved for site 328.\n",
      "Weather data saved for site 329.\n",
      "Weather data saved for site 330.\n",
      "Weather data saved for site 331.\n",
      "Weather data saved for site 332.\n",
      "Weather data saved for site 333.\n",
      "Weather data saved for site 334.\n",
      "Weather data saved for site 335.\n",
      "Weather data saved for site 336.\n",
      "Weather data saved for site 337.\n",
      "Weather data saved for site 338.\n",
      "Weather data saved for site 339.\n",
      "Weather data saved for site 340.\n",
      "Weather data saved for site 341.\n",
      "Weather data saved for site 342.\n",
      "Weather data saved for site 343.\n",
      "Weather data saved for site 344.\n",
      "Weather data saved for site 345.\n",
      "Weather data saved for site 346.\n",
      "Weather data saved for site 347.\n",
      "Weather data saved for site 348.\n",
      "Weather data saved for site 349.\n",
      "Weather data saved for site 350.\n",
      "Weather data saved for site 351.\n",
      "Weather data saved for site 352.\n",
      "Weather data saved for site 353.\n",
      "Weather data saved for site 354.\n",
      "Weather data saved for site 355.\n",
      "Weather data saved for site 356.\n",
      "Weather data saved for site 357.\n",
      "Weather data saved for site 358.\n",
      "Weather data saved for site 359.\n",
      "Weather data saved for site 360.\n",
      "Weather data saved for site 361.\n",
      "Weather data saved for site 362.\n",
      "Weather data saved for site 363.\n",
      "Weather data saved for site 364.\n",
      "Weather data saved for site 365.\n",
      "Weather data saved for site 366.\n",
      "Weather data saved for site 367.\n",
      "Weather data saved for site 368.\n",
      "Weather data saved for site 369.\n",
      "Weather data saved for site 370.\n",
      "Weather data saved for site 371.\n",
      "Weather data saved for site 372.\n",
      "Weather data saved for site 373.\n",
      "Weather data saved for site 374.\n",
      "Weather data saved for site 375.\n",
      "Weather data saved for site 376.\n",
      "Weather data saved for site 377.\n",
      "Weather data saved for site 378.\n",
      "Weather data saved for site 379.\n",
      "Weather data saved for site 380.\n",
      "Weather data saved for site 381.\n",
      "Weather data saved for site 382.\n",
      "Weather data saved for site 383.\n",
      "Weather data saved for site 384.\n",
      "Weather data saved for site 385.\n",
      "Weather data saved for site 386.\n",
      "Weather data saved for site 387.\n",
      "Weather data saved for site 388.\n",
      "Weather data saved for site 389.\n",
      "Weather data saved for site 390.\n",
      "Weather data saved for site 391.\n",
      "Weather data saved for site 392.\n",
      "Weather data saved for site 393.\n",
      "Weather data saved for site 394.\n",
      "Weather data saved for site 395.\n",
      "Weather data saved for site 396.\n",
      "Weather data saved for site 397.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T15:46:39.865867Z",
     "start_time": "2025-09-28T15:46:36.087949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from aquacrop.utils.prepare_weather import prepare_weather\n",
    "\n",
    "# Define the directory containing the weather files\n",
    "input_dir = \"./ClimateData\"\n",
    "prepared_wdf_dir = \"./PreparedWDF\"  # Directory to save prepared wdf\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(prepared_wdf_dir, exist_ok=True)\n",
    "\n",
    "# Number of sites\n",
    "num_sites = 397  # Adjust as necessary\n",
    "\n",
    "# Create a list of climate file names dynamically\n",
    "climate_files = [f\"site_{i}.txt\" for i in range(1, num_sites + 1)]  # e.g., site_1.txt, site_2.txt, etc.\n",
    "\n",
    "# Loop through each file name\n",
    "for climate_file in climate_files:\n",
    "    file_path = os.path.join(input_dir, climate_file)  # Construct the full file path\n",
    "\n",
    "    if os.path.exists(file_path):  # Check if the file exists\n",
    "        try:\n",
    "            # Prepare weather data\n",
    "            wdf = prepare_weather(file_path)  \n",
    "\n",
    "            # Save the prepared weather data to CSV\n",
    "            prepared_wdf_path = os.path.join(prepared_wdf_dir, f\"prepared_{climate_file}\")\n",
    "            wdf.to_csv(prepared_wdf_path, index=False)  # Assuming wdf can be converted to CSV\n",
    "\n",
    "            print(f\"Weather data prepared and saved for {climate_file}.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {climate_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n"
   ],
   "id": "2b44ae9fa9a722d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data prepared and saved for site_1.txt.\n",
      "Weather data prepared and saved for site_2.txt.\n",
      "Weather data prepared and saved for site_3.txt.\n",
      "Weather data prepared and saved for site_4.txt.\n",
      "Weather data prepared and saved for site_5.txt.\n",
      "Weather data prepared and saved for site_6.txt.\n",
      "Weather data prepared and saved for site_7.txt.\n",
      "Weather data prepared and saved for site_8.txt.\n",
      "Weather data prepared and saved for site_9.txt.\n",
      "Weather data prepared and saved for site_10.txt.\n",
      "Weather data prepared and saved for site_11.txt.\n",
      "Weather data prepared and saved for site_12.txt.\n",
      "Weather data prepared and saved for site_13.txt.\n",
      "Weather data prepared and saved for site_14.txt.\n",
      "Weather data prepared and saved for site_15.txt.\n",
      "Weather data prepared and saved for site_16.txt.\n",
      "Weather data prepared and saved for site_17.txt.\n",
      "Weather data prepared and saved for site_18.txt.\n",
      "Weather data prepared and saved for site_19.txt.\n",
      "Weather data prepared and saved for site_20.txt.\n",
      "Weather data prepared and saved for site_21.txt.\n",
      "Weather data prepared and saved for site_22.txt.\n",
      "Weather data prepared and saved for site_23.txt.\n",
      "Weather data prepared and saved for site_24.txt.\n",
      "Weather data prepared and saved for site_25.txt.\n",
      "Weather data prepared and saved for site_26.txt.\n",
      "Weather data prepared and saved for site_27.txt.\n",
      "Weather data prepared and saved for site_28.txt.\n",
      "Weather data prepared and saved for site_29.txt.\n",
      "Weather data prepared and saved for site_30.txt.\n",
      "Weather data prepared and saved for site_31.txt.\n",
      "Weather data prepared and saved for site_32.txt.\n",
      "Weather data prepared and saved for site_33.txt.\n",
      "Weather data prepared and saved for site_34.txt.\n",
      "Weather data prepared and saved for site_35.txt.\n",
      "Weather data prepared and saved for site_36.txt.\n",
      "Weather data prepared and saved for site_37.txt.\n",
      "Weather data prepared and saved for site_38.txt.\n",
      "Weather data prepared and saved for site_39.txt.\n",
      "Weather data prepared and saved for site_40.txt.\n",
      "Weather data prepared and saved for site_41.txt.\n",
      "Weather data prepared and saved for site_42.txt.\n",
      "Weather data prepared and saved for site_43.txt.\n",
      "Weather data prepared and saved for site_44.txt.\n",
      "Weather data prepared and saved for site_45.txt.\n",
      "Weather data prepared and saved for site_46.txt.\n",
      "Weather data prepared and saved for site_47.txt.\n",
      "Weather data prepared and saved for site_48.txt.\n",
      "Weather data prepared and saved for site_49.txt.\n",
      "Weather data prepared and saved for site_50.txt.\n",
      "Weather data prepared and saved for site_51.txt.\n",
      "Weather data prepared and saved for site_52.txt.\n",
      "Weather data prepared and saved for site_53.txt.\n",
      "Weather data prepared and saved for site_54.txt.\n",
      "Weather data prepared and saved for site_55.txt.\n",
      "Weather data prepared and saved for site_56.txt.\n",
      "Weather data prepared and saved for site_57.txt.\n",
      "Weather data prepared and saved for site_58.txt.\n",
      "Weather data prepared and saved for site_59.txt.\n",
      "Weather data prepared and saved for site_60.txt.\n",
      "Weather data prepared and saved for site_61.txt.\n",
      "Weather data prepared and saved for site_62.txt.\n",
      "Weather data prepared and saved for site_63.txt.\n",
      "Weather data prepared and saved for site_64.txt.\n",
      "Weather data prepared and saved for site_65.txt.\n",
      "Weather data prepared and saved for site_66.txt.\n",
      "Weather data prepared and saved for site_67.txt.\n",
      "Weather data prepared and saved for site_68.txt.\n",
      "Weather data prepared and saved for site_69.txt.\n",
      "Weather data prepared and saved for site_70.txt.\n",
      "Weather data prepared and saved for site_71.txt.\n",
      "Weather data prepared and saved for site_72.txt.\n",
      "Weather data prepared and saved for site_73.txt.\n",
      "Weather data prepared and saved for site_74.txt.\n",
      "Weather data prepared and saved for site_75.txt.\n",
      "Weather data prepared and saved for site_76.txt.\n",
      "Weather data prepared and saved for site_77.txt.\n",
      "Weather data prepared and saved for site_78.txt.\n",
      "Weather data prepared and saved for site_79.txt.\n",
      "Weather data prepared and saved for site_80.txt.\n",
      "Weather data prepared and saved for site_81.txt.\n",
      "Weather data prepared and saved for site_82.txt.\n",
      "Weather data prepared and saved for site_83.txt.\n",
      "Weather data prepared and saved for site_84.txt.\n",
      "Weather data prepared and saved for site_85.txt.\n",
      "Weather data prepared and saved for site_86.txt.\n",
      "Weather data prepared and saved for site_87.txt.\n",
      "Weather data prepared and saved for site_88.txt.\n",
      "Weather data prepared and saved for site_89.txt.\n",
      "Weather data prepared and saved for site_90.txt.\n",
      "Weather data prepared and saved for site_91.txt.\n",
      "Weather data prepared and saved for site_92.txt.\n",
      "Weather data prepared and saved for site_93.txt.\n",
      "Weather data prepared and saved for site_94.txt.\n",
      "Weather data prepared and saved for site_95.txt.\n",
      "Weather data prepared and saved for site_96.txt.\n",
      "Weather data prepared and saved for site_97.txt.\n",
      "Weather data prepared and saved for site_98.txt.\n",
      "Weather data prepared and saved for site_99.txt.\n",
      "Weather data prepared and saved for site_100.txt.\n",
      "Weather data prepared and saved for site_101.txt.\n",
      "Weather data prepared and saved for site_102.txt.\n",
      "Weather data prepared and saved for site_103.txt.\n",
      "Weather data prepared and saved for site_104.txt.\n",
      "Weather data prepared and saved for site_105.txt.\n",
      "Weather data prepared and saved for site_106.txt.\n",
      "Weather data prepared and saved for site_107.txt.\n",
      "Weather data prepared and saved for site_108.txt.\n",
      "Weather data prepared and saved for site_109.txt.\n",
      "Weather data prepared and saved for site_110.txt.\n",
      "Weather data prepared and saved for site_111.txt.\n",
      "Weather data prepared and saved for site_112.txt.\n",
      "Weather data prepared and saved for site_113.txt.\n",
      "Weather data prepared and saved for site_114.txt.\n",
      "Weather data prepared and saved for site_115.txt.\n",
      "Weather data prepared and saved for site_116.txt.\n",
      "Weather data prepared and saved for site_117.txt.\n",
      "Weather data prepared and saved for site_118.txt.\n",
      "Weather data prepared and saved for site_119.txt.\n",
      "Weather data prepared and saved for site_120.txt.\n",
      "Weather data prepared and saved for site_121.txt.\n",
      "Weather data prepared and saved for site_122.txt.\n",
      "Weather data prepared and saved for site_123.txt.\n",
      "Weather data prepared and saved for site_124.txt.\n",
      "Weather data prepared and saved for site_125.txt.\n",
      "Weather data prepared and saved for site_126.txt.\n",
      "Weather data prepared and saved for site_127.txt.\n",
      "Weather data prepared and saved for site_128.txt.\n",
      "Weather data prepared and saved for site_129.txt.\n",
      "Weather data prepared and saved for site_130.txt.\n",
      "Weather data prepared and saved for site_131.txt.\n",
      "Weather data prepared and saved for site_132.txt.\n",
      "Weather data prepared and saved for site_133.txt.\n",
      "Weather data prepared and saved for site_134.txt.\n",
      "Weather data prepared and saved for site_135.txt.\n",
      "Weather data prepared and saved for site_136.txt.\n",
      "Weather data prepared and saved for site_137.txt.\n",
      "Weather data prepared and saved for site_138.txt.\n",
      "Weather data prepared and saved for site_139.txt.\n",
      "Weather data prepared and saved for site_140.txt.\n",
      "Weather data prepared and saved for site_141.txt.\n",
      "Weather data prepared and saved for site_142.txt.\n",
      "Weather data prepared and saved for site_143.txt.\n",
      "Weather data prepared and saved for site_144.txt.\n",
      "Weather data prepared and saved for site_145.txt.\n",
      "Weather data prepared and saved for site_146.txt.\n",
      "Weather data prepared and saved for site_147.txt.\n",
      "Weather data prepared and saved for site_148.txt.\n",
      "Weather data prepared and saved for site_149.txt.\n",
      "Weather data prepared and saved for site_150.txt.\n",
      "Weather data prepared and saved for site_151.txt.\n",
      "Weather data prepared and saved for site_152.txt.\n",
      "Weather data prepared and saved for site_153.txt.\n",
      "Weather data prepared and saved for site_154.txt.\n",
      "Weather data prepared and saved for site_155.txt.\n",
      "Weather data prepared and saved for site_156.txt.\n",
      "Weather data prepared and saved for site_157.txt.\n",
      "Weather data prepared and saved for site_158.txt.\n",
      "Weather data prepared and saved for site_159.txt.\n",
      "Weather data prepared and saved for site_160.txt.\n",
      "Weather data prepared and saved for site_161.txt.\n",
      "Weather data prepared and saved for site_162.txt.\n",
      "Weather data prepared and saved for site_163.txt.\n",
      "Weather data prepared and saved for site_164.txt.\n",
      "Weather data prepared and saved for site_165.txt.\n",
      "Weather data prepared and saved for site_166.txt.\n",
      "Weather data prepared and saved for site_167.txt.\n",
      "Weather data prepared and saved for site_168.txt.\n",
      "Weather data prepared and saved for site_169.txt.\n",
      "Weather data prepared and saved for site_170.txt.\n",
      "Weather data prepared and saved for site_171.txt.\n",
      "Weather data prepared and saved for site_172.txt.\n",
      "Weather data prepared and saved for site_173.txt.\n",
      "Weather data prepared and saved for site_174.txt.\n",
      "Weather data prepared and saved for site_175.txt.\n",
      "Weather data prepared and saved for site_176.txt.\n",
      "Weather data prepared and saved for site_177.txt.\n",
      "Weather data prepared and saved for site_178.txt.\n",
      "Weather data prepared and saved for site_179.txt.\n",
      "Weather data prepared and saved for site_180.txt.\n",
      "Weather data prepared and saved for site_181.txt.\n",
      "Weather data prepared and saved for site_182.txt.\n",
      "Weather data prepared and saved for site_183.txt.\n",
      "Weather data prepared and saved for site_184.txt.\n",
      "Weather data prepared and saved for site_185.txt.\n",
      "Weather data prepared and saved for site_186.txt.\n",
      "Weather data prepared and saved for site_187.txt.\n",
      "Weather data prepared and saved for site_188.txt.\n",
      "Weather data prepared and saved for site_189.txt.\n",
      "Weather data prepared and saved for site_190.txt.\n",
      "Weather data prepared and saved for site_191.txt.\n",
      "Weather data prepared and saved for site_192.txt.\n",
      "Weather data prepared and saved for site_193.txt.\n",
      "Weather data prepared and saved for site_194.txt.\n",
      "Weather data prepared and saved for site_195.txt.\n",
      "Weather data prepared and saved for site_196.txt.\n",
      "Weather data prepared and saved for site_197.txt.\n",
      "Weather data prepared and saved for site_198.txt.\n",
      "Weather data prepared and saved for site_199.txt.\n",
      "Weather data prepared and saved for site_200.txt.\n",
      "Weather data prepared and saved for site_201.txt.\n",
      "Weather data prepared and saved for site_202.txt.\n",
      "Weather data prepared and saved for site_203.txt.\n",
      "Weather data prepared and saved for site_204.txt.\n",
      "Weather data prepared and saved for site_205.txt.\n",
      "Weather data prepared and saved for site_206.txt.\n",
      "Weather data prepared and saved for site_207.txt.\n",
      "Weather data prepared and saved for site_208.txt.\n",
      "Weather data prepared and saved for site_209.txt.\n",
      "Weather data prepared and saved for site_210.txt.\n",
      "Weather data prepared and saved for site_211.txt.\n",
      "Weather data prepared and saved for site_212.txt.\n",
      "Weather data prepared and saved for site_213.txt.\n",
      "Weather data prepared and saved for site_214.txt.\n",
      "Weather data prepared and saved for site_215.txt.\n",
      "Weather data prepared and saved for site_216.txt.\n",
      "Weather data prepared and saved for site_217.txt.\n",
      "Weather data prepared and saved for site_218.txt.\n",
      "Weather data prepared and saved for site_219.txt.\n",
      "Weather data prepared and saved for site_220.txt.\n",
      "Weather data prepared and saved for site_221.txt.\n",
      "Weather data prepared and saved for site_222.txt.\n",
      "Weather data prepared and saved for site_223.txt.\n",
      "Weather data prepared and saved for site_224.txt.\n",
      "Weather data prepared and saved for site_225.txt.\n",
      "Weather data prepared and saved for site_226.txt.\n",
      "Weather data prepared and saved for site_227.txt.\n",
      "Weather data prepared and saved for site_228.txt.\n",
      "Weather data prepared and saved for site_229.txt.\n",
      "Weather data prepared and saved for site_230.txt.\n",
      "Weather data prepared and saved for site_231.txt.\n",
      "Weather data prepared and saved for site_232.txt.\n",
      "Weather data prepared and saved for site_233.txt.\n",
      "Weather data prepared and saved for site_234.txt.\n",
      "Weather data prepared and saved for site_235.txt.\n",
      "Weather data prepared and saved for site_236.txt.\n",
      "Weather data prepared and saved for site_237.txt.\n",
      "Weather data prepared and saved for site_238.txt.\n",
      "Weather data prepared and saved for site_239.txt.\n",
      "Weather data prepared and saved for site_240.txt.\n",
      "Weather data prepared and saved for site_241.txt.\n",
      "Weather data prepared and saved for site_242.txt.\n",
      "Weather data prepared and saved for site_243.txt.\n",
      "Weather data prepared and saved for site_244.txt.\n",
      "Weather data prepared and saved for site_245.txt.\n",
      "Weather data prepared and saved for site_246.txt.\n",
      "Weather data prepared and saved for site_247.txt.\n",
      "Weather data prepared and saved for site_248.txt.\n",
      "Weather data prepared and saved for site_249.txt.\n",
      "Weather data prepared and saved for site_250.txt.\n",
      "Weather data prepared and saved for site_251.txt.\n",
      "Weather data prepared and saved for site_252.txt.\n",
      "Weather data prepared and saved for site_253.txt.\n",
      "Weather data prepared and saved for site_254.txt.\n",
      "Weather data prepared and saved for site_255.txt.\n",
      "Weather data prepared and saved for site_256.txt.\n",
      "Weather data prepared and saved for site_257.txt.\n",
      "Weather data prepared and saved for site_258.txt.\n",
      "Weather data prepared and saved for site_259.txt.\n",
      "Weather data prepared and saved for site_260.txt.\n",
      "Weather data prepared and saved for site_261.txt.\n",
      "Weather data prepared and saved for site_262.txt.\n",
      "Weather data prepared and saved for site_263.txt.\n",
      "Weather data prepared and saved for site_264.txt.\n",
      "Weather data prepared and saved for site_265.txt.\n",
      "Weather data prepared and saved for site_266.txt.\n",
      "Weather data prepared and saved for site_267.txt.\n",
      "Weather data prepared and saved for site_268.txt.\n",
      "Weather data prepared and saved for site_269.txt.\n",
      "Weather data prepared and saved for site_270.txt.\n",
      "Weather data prepared and saved for site_271.txt.\n",
      "Weather data prepared and saved for site_272.txt.\n",
      "Weather data prepared and saved for site_273.txt.\n",
      "Weather data prepared and saved for site_274.txt.\n",
      "Weather data prepared and saved for site_275.txt.\n",
      "Weather data prepared and saved for site_276.txt.\n",
      "Weather data prepared and saved for site_277.txt.\n",
      "Weather data prepared and saved for site_278.txt.\n",
      "Weather data prepared and saved for site_279.txt.\n",
      "Weather data prepared and saved for site_280.txt.\n",
      "Weather data prepared and saved for site_281.txt.\n",
      "Weather data prepared and saved for site_282.txt.\n",
      "Weather data prepared and saved for site_283.txt.\n",
      "Weather data prepared and saved for site_284.txt.\n",
      "Weather data prepared and saved for site_285.txt.\n",
      "Weather data prepared and saved for site_286.txt.\n",
      "Weather data prepared and saved for site_287.txt.\n",
      "Weather data prepared and saved for site_288.txt.\n",
      "Weather data prepared and saved for site_289.txt.\n",
      "Weather data prepared and saved for site_290.txt.\n",
      "Weather data prepared and saved for site_291.txt.\n",
      "Weather data prepared and saved for site_292.txt.\n",
      "Weather data prepared and saved for site_293.txt.\n",
      "Weather data prepared and saved for site_294.txt.\n",
      "Weather data prepared and saved for site_295.txt.\n",
      "Weather data prepared and saved for site_296.txt.\n",
      "Weather data prepared and saved for site_297.txt.\n",
      "Weather data prepared and saved for site_298.txt.\n",
      "Weather data prepared and saved for site_299.txt.\n",
      "Weather data prepared and saved for site_300.txt.\n",
      "Weather data prepared and saved for site_301.txt.\n",
      "Weather data prepared and saved for site_302.txt.\n",
      "Weather data prepared and saved for site_303.txt.\n",
      "Weather data prepared and saved for site_304.txt.\n",
      "Weather data prepared and saved for site_305.txt.\n",
      "Weather data prepared and saved for site_306.txt.\n",
      "Weather data prepared and saved for site_307.txt.\n",
      "Weather data prepared and saved for site_308.txt.\n",
      "Weather data prepared and saved for site_309.txt.\n",
      "Weather data prepared and saved for site_310.txt.\n",
      "Weather data prepared and saved for site_311.txt.\n",
      "Weather data prepared and saved for site_312.txt.\n",
      "Weather data prepared and saved for site_313.txt.\n",
      "Weather data prepared and saved for site_314.txt.\n",
      "Weather data prepared and saved for site_315.txt.\n",
      "Weather data prepared and saved for site_316.txt.\n",
      "Weather data prepared and saved for site_317.txt.\n",
      "Weather data prepared and saved for site_318.txt.\n",
      "Weather data prepared and saved for site_319.txt.\n",
      "Weather data prepared and saved for site_320.txt.\n",
      "Weather data prepared and saved for site_321.txt.\n",
      "Weather data prepared and saved for site_322.txt.\n",
      "Weather data prepared and saved for site_323.txt.\n",
      "Weather data prepared and saved for site_324.txt.\n",
      "Weather data prepared and saved for site_325.txt.\n",
      "Weather data prepared and saved for site_326.txt.\n",
      "Weather data prepared and saved for site_327.txt.\n",
      "Weather data prepared and saved for site_328.txt.\n",
      "Weather data prepared and saved for site_329.txt.\n",
      "Weather data prepared and saved for site_330.txt.\n",
      "Weather data prepared and saved for site_331.txt.\n",
      "Weather data prepared and saved for site_332.txt.\n",
      "Weather data prepared and saved for site_333.txt.\n",
      "Weather data prepared and saved for site_334.txt.\n",
      "Weather data prepared and saved for site_335.txt.\n",
      "Weather data prepared and saved for site_336.txt.\n",
      "Weather data prepared and saved for site_337.txt.\n",
      "Weather data prepared and saved for site_338.txt.\n",
      "Weather data prepared and saved for site_339.txt.\n",
      "Weather data prepared and saved for site_340.txt.\n",
      "Weather data prepared and saved for site_341.txt.\n",
      "Weather data prepared and saved for site_342.txt.\n",
      "Weather data prepared and saved for site_343.txt.\n",
      "Weather data prepared and saved for site_344.txt.\n",
      "Weather data prepared and saved for site_345.txt.\n",
      "Weather data prepared and saved for site_346.txt.\n",
      "Weather data prepared and saved for site_347.txt.\n",
      "Weather data prepared and saved for site_348.txt.\n",
      "Weather data prepared and saved for site_349.txt.\n",
      "Weather data prepared and saved for site_350.txt.\n",
      "Weather data prepared and saved for site_351.txt.\n",
      "Weather data prepared and saved for site_352.txt.\n",
      "Weather data prepared and saved for site_353.txt.\n",
      "Weather data prepared and saved for site_354.txt.\n",
      "Weather data prepared and saved for site_355.txt.\n",
      "Weather data prepared and saved for site_356.txt.\n",
      "Weather data prepared and saved for site_357.txt.\n",
      "Weather data prepared and saved for site_358.txt.\n",
      "Weather data prepared and saved for site_359.txt.\n",
      "Weather data prepared and saved for site_360.txt.\n",
      "Weather data prepared and saved for site_361.txt.\n",
      "Weather data prepared and saved for site_362.txt.\n",
      "Weather data prepared and saved for site_363.txt.\n",
      "Weather data prepared and saved for site_364.txt.\n",
      "Weather data prepared and saved for site_365.txt.\n",
      "Weather data prepared and saved for site_366.txt.\n",
      "Weather data prepared and saved for site_367.txt.\n",
      "Weather data prepared and saved for site_368.txt.\n",
      "Weather data prepared and saved for site_369.txt.\n",
      "Weather data prepared and saved for site_370.txt.\n",
      "Weather data prepared and saved for site_371.txt.\n",
      "Weather data prepared and saved for site_372.txt.\n",
      "Weather data prepared and saved for site_373.txt.\n",
      "Weather data prepared and saved for site_374.txt.\n",
      "Weather data prepared and saved for site_375.txt.\n",
      "Weather data prepared and saved for site_376.txt.\n",
      "Weather data prepared and saved for site_377.txt.\n",
      "Weather data prepared and saved for site_378.txt.\n",
      "Weather data prepared and saved for site_379.txt.\n",
      "Weather data prepared and saved for site_380.txt.\n",
      "Weather data prepared and saved for site_381.txt.\n",
      "Weather data prepared and saved for site_382.txt.\n",
      "Weather data prepared and saved for site_383.txt.\n",
      "Weather data prepared and saved for site_384.txt.\n",
      "Weather data prepared and saved for site_385.txt.\n",
      "Weather data prepared and saved for site_386.txt.\n",
      "Weather data prepared and saved for site_387.txt.\n",
      "Weather data prepared and saved for site_388.txt.\n",
      "Weather data prepared and saved for site_389.txt.\n",
      "Weather data prepared and saved for site_390.txt.\n",
      "Weather data prepared and saved for site_391.txt.\n",
      "Weather data prepared and saved for site_392.txt.\n",
      "Weather data prepared and saved for site_393.txt.\n",
      "Weather data prepared and saved for site_394.txt.\n",
      "Weather data prepared and saved for site_395.txt.\n",
      "Weather data prepared and saved for site_396.txt.\n",
      "Weather data prepared and saved for site_397.txt.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T15:48:44.905902Z",
     "start_time": "2025-09-28T15:48:43.455332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# At this point, wdf_list will contain all the prepared weather data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from aquacrop import AquaCropModel, Soil, Crop, InitialWaterContent, IrrigationManagement\n",
    "from tqdm import tqdm\n",
    "from scipy.optimize import fmin  # Ensure fmin is imported for optimization\n",
    "\n",
    "\n",
    "# Prepare the weather data\n",
    "\n",
    "#wdf1 = prepare_weather(\"ClimateData/champion_climate1.txt\")\n",
    "#wdf2 = prepare_weather(\"ClimateData/champion_climate2.txt\")\n",
    "\n",
    "def run_model(smts, max_irr_season, year1, year2, wdf):\n",
    "    \"\"\"\n",
    "    Function to run model and return results for a given set of soil moisture targets.\n",
    "    \"\"\"\n",
    "    wheat = Crop('Wheat',\n",
    "                planting_date='05/01',\n",
    "                harvest_date='10/30',\n",
    "                CropType=3,  # Conservative parameters\n",
    "                Tbase=5,\n",
    "                Tupp=35,\n",
    "                #Maturity=975,\n",
    "                #Emergence=42,\n",
    "                #MaxRooting=597,\n",
    "                #Flowering=114,\n",
    "                #Senescence=821,\n",
    "                #HIstart=224,\n",
    "                Zmax=0.7,\n",
    "                WP=16,\n",
    "                Tmin_up=8,\n",
    "                Tmax_lo=40,\n",
    "                exc=50,\n",
    "                CGC=0.16764,\n",
    "                CCx=0.95,\n",
    "                CDC=0.13653,\n",
    "                #SeedSize=5,\n",
    "                Kcb=1.10,\n",
    "                fshape_r=15,\n",
    "                SxTopQ=0.020,\n",
    "                SxBotQ=0.005,\n",
    "                p_up4=0.8,\n",
    "                p_up2=0.55,\n",
    "                fshape_w1=4,\n",
    "                PlantPop=2000000\n",
    "                )  # Plant population (plants/ha))  # Define crop\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    soil = Soil('LoamySand')  # Define soil\n",
    "    init_wc = InitialWaterContent(wc_type='Pct', value=[70])  # Define initial soil water conditions\n",
    "\n",
    "    irrmngt = IrrigationManagement(irrigation_method=1, SMT=smts, MaxIrrSeason=max_irr_season)  # Define irrigation management\n",
    "\n",
    "    # Create and run model\n",
    "    model = AquaCropModel(f'{year1}/05/01', f'{year2}/10/31', wdf, soil, wheat,\n",
    "                          irrigation_management=irrmngt, initial_water_content=init_wc)\n",
    "\n",
    "    model.run_model(till_termination=True)\n",
    "    return model.get_simulation_results()\n",
    "\n",
    "def evaluate(smts, max_irr_season, wdf, test=False):\n",
    "    \"\"\"\n",
    "    Function to run model and calculate reward (yield) for a given set of soil moisture targets.\n",
    "    \"\"\"\n",
    "    # Run model # year chnage ### year chnage\n",
    "    out = run_model(smts, max_irr_season, year1=2018, year2=2018, wdf=wdf)\n",
    "\n",
    "    # Get yields and total irrigation\n",
    "    yld = out['Fresh yield (tonne/ha)'].mean()\n",
    "    tirr = out['Seasonal irrigation (mm)'].mean()\n",
    "\n",
    "    reward = yld\n",
    "\n",
    "    # Return either the negative reward (for the optimization)\n",
    "    # or the yield and total irrigation (for analysis)\n",
    "    if test:\n",
    "        return yld, tirr, reward\n",
    "    else:\n",
    "        return -reward\n",
    "\n",
    "# Modify get_starting_point to accept wdf\n",
    "def get_starting_point(num_smts, max_irr_season, num_searches, wdf):\n",
    "    \"\"\"\n",
    "    Find good starting threshold(s) for optimization.\n",
    "    \"\"\"\n",
    "    # Get random SMT's\n",
    "    x0list = np.random.rand(num_searches, num_smts) * 100\n",
    "    rlist = []\n",
    "\n",
    "    # Evaluate random SMT's\n",
    "    for xtest in x0list:\n",
    "        r = evaluate(xtest, max_irr_season, wdf)\n",
    "        rlist.append(r)\n",
    "\n",
    "    # Save best SMT\n",
    "    x0 = x0list[np.argmin(rlist)]\n",
    "    return x0\n",
    "\n",
    "# Modify optimize to accept wdf\n",
    "def optimize(num_smts, max_irr_season, wdf, num_searches=100):\n",
    "    \"\"\"\n",
    "    Optimize thresholds to be profit-maximizing.\n",
    "    \"\"\"\n",
    "    # Get starting optimization strategy\n",
    "    x0 = get_starting_point(num_smts, max_irr_season, num_searches, wdf)\n",
    "\n",
    "    # Run optimization\n",
    "    res = fmin(evaluate, x0, disp=0, args=(max_irr_season, wdf))\n",
    "\n",
    "    # Reshape array\n",
    "    smts = res.squeeze()\n",
    "\n",
    "    # Evaluate optimal strategy\n",
    "    return smts\n",
    "\n"
   ],
   "id": "31341e92ab4002af",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T15:49:00.757880Z",
     "start_time": "2025-09-28T15:49:00.755224Z"
    }
   },
   "cell_type": "code",
   "source": "max_irr_values = [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]",
   "id": "370243b0718ba016",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-28T16:02:14.281689Z",
     "start_time": "2025-09-28T15:50:20.844043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(1, 20)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_1.csv\", index=False)\n"
   ],
   "id": "42064e268dd75722",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 28\u001B[0m\n\u001B[1;32m     25\u001B[0m results_list \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Parallel processing using joblib\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m all_results \u001B[38;5;241m=\u001B[39m \u001B[43mParallel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdelayed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprocess_site_irr_max\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43msite_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_irr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwdfs\u001B[49m\u001B[43m[\u001B[49m\u001B[43msite_id\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmax_irr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mmax_irr_values\u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msite_id\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msite_ids\u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;66;03m# Convert the list of results to a DataFrame\u001B[39;00m\n\u001B[1;32m     35\u001B[0m results_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(all_results)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:2007\u001B[0m, in \u001B[0;36mParallel.__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   2001\u001B[0m \u001B[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001B[39;00m\n\u001B[1;32m   2002\u001B[0m \u001B[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001B[39;00m\n\u001B[1;32m   2003\u001B[0m \u001B[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001B[39;00m\n\u001B[1;32m   2004\u001B[0m \u001B[38;5;66;03m# dispatch of the tasks to the workers.\u001B[39;00m\n\u001B[1;32m   2005\u001B[0m \u001B[38;5;28mnext\u001B[39m(output)\n\u001B[0;32m-> 2007\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_generator \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1650\u001B[0m, in \u001B[0;36mParallel._get_outputs\u001B[0;34m(self, iterator, pre_dispatch)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m   1649\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backend\u001B[38;5;241m.\u001B[39mretrieval_context():\n\u001B[0;32m-> 1650\u001B[0m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_retrieve()\n\u001B[1;32m   1652\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mGeneratorExit\u001B[39;00m:\n\u001B[1;32m   1653\u001B[0m     \u001B[38;5;66;03m# The generator has been garbage collected before being fully\u001B[39;00m\n\u001B[1;32m   1654\u001B[0m     \u001B[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001B[39;00m\n\u001B[1;32m   1655\u001B[0m     \u001B[38;5;66;03m# the user if necessary.\u001B[39;00m\n\u001B[1;32m   1656\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/joblib/parallel.py:1762\u001B[0m, in \u001B[0;36mParallel._retrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# async callbacks to progress.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ((\u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m\n\u001B[1;32m   1760\u001B[0m     (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jobs[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mget_status(\n\u001B[1;32m   1761\u001B[0m         timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout) \u001B[38;5;241m==\u001B[39m TASK_PENDING)):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1763\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m \u001B[38;5;66;03m# We need to be careful: the job list can be filling up as\u001B[39;00m\n\u001B[1;32m   1766\u001B[0m \u001B[38;5;66;03m# we empty it and Python list are not thread-safe by\u001B[39;00m\n\u001B[1;32m   1767\u001B[0m \u001B[38;5;66;03m# default hence the use of the lock\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(20, 30)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_2.csv\",\n",
    "    index=False)"
   ],
   "id": "9433287ae9af6e92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(30, 40)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_3.csv\",\n",
    "    index=False)"
   ],
   "id": "2ff8c7abc863f5b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(40, 50)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_4.csv\",\n",
    "    index=False)"
   ],
   "id": "6aa40bec81492c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(50, 60)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_5.csv\",\n",
    "    index=False)"
   ],
   "id": "18ef6d801c8f47a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(60, 70)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_6.csv\",\n",
    "    index=False)\n"
   ],
   "id": "9410a8544f16352c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(70, 80)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_7.csv\",\n",
    "    index=False)\n"
   ],
   "id": "9ba77fc45108effb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(80, 90)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_8.csv\",\n",
    "    index=False)\n"
   ],
   "id": "7482b8c9b67f6045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(90, 100)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_9.csv\",\n",
    "    index=False)\n"
   ],
   "id": "b5ff3a4e846cae3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(100, 110)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_10.csv\",\n",
    "    index=False)\n"
   ],
   "id": "465b4c795ed57b34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(110, 120)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_11.csv\",\n",
    "    index=False)\n"
   ],
   "id": "a732ace6c3532008",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(120, 130)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_12.csv\",\n",
    "    index=False)\n"
   ],
   "id": "7bfa3287298dd76e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(130, 140)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_13.csv\",\n",
    "    index=False)\n"
   ],
   "id": "1984d15daa0017c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(140, 150)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_14.csv\",\n",
    "    index=False)\n"
   ],
   "id": "49e592ca7ab69571",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(150, 160)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_15.csv\",\n",
    "    index=False)\n"
   ],
   "id": "fa54e012b529373a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(160, 170)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_16.csv\",\n",
    "    index=False)"
   ],
   "id": "cb2322cac2fc8992",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(170, 180)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_17.csv\",\n",
    "    index=False)\n"
   ],
   "id": "b3d6740fd84f2b39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(180, 190)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_18.csv\",\n",
    "    index=False)\n"
   ],
   "id": "53d3de5ea9a35c7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(190, 200)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_19.csv\",\n",
    "    index=False)\n"
   ],
   "id": "c486a626029d67d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(200, 210)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_20.csv\",\n",
    "    index=False)\n"
   ],
   "id": "94feabb7414da579",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(210, 220)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_21.csv\",\n",
    "    index=False)\n"
   ],
   "id": "ddc8cc77e68618fe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(220, 230)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_22.csv\",\n",
    "    index=False)\n"
   ],
   "id": "fef0973e8a2563fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(230, 240)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_23.csv\",\n",
    "    index=False)\n"
   ],
   "id": "a52d99c7778f96b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(240, 250)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_24.csv\",\n",
    "    index=False)\n"
   ],
   "id": "422cbcc00b8b81a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(250, 260)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_25.csv\",\n",
    "    index=False)\n"
   ],
   "id": "7d7d6f16a92b66a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(260, 270)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_26.csv\",\n",
    "    index=False)\n"
   ],
   "id": "abd3d6e7fd0f322d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(270, 280)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_27.csv\",\n",
    "    index=False)\n"
   ],
   "id": "b2734d91ff71e0e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(280, 290)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_28.csv\",\n",
    "    index=False)\n"
   ],
   "id": "b315cd98413074f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(290, 300)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_29.csv\",\n",
    "    index=False)\n"
   ],
   "id": "1b76900e1ac3459b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(300, 310)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_30.csv\",\n",
    "    index=False)\n"
   ],
   "id": "e80314b650a80f8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(310, 320)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_31.csv\",\n",
    "    index=False)\n"
   ],
   "id": "b37580b95c40eb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(320, 330)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_32.csv\",\n",
    "    index=False)\n"
   ],
   "id": "bb74bc99a111168d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(330, 340)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_33.csv\",\n",
    "    index=False)\n"
   ],
   "id": "5be1e1bd09d583f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(340, 350)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_34.csv\",\n",
    "    index=False)\n"
   ],
   "id": "83f842cc4bdc9a63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(350, 360)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_35.csv\",\n",
    "    index=False)\n"
   ],
   "id": "b0b2b3bd055af307",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(360, 370)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_36.csv\",\n",
    "    index=False)\n"
   ],
   "id": "cfcbf2fc6149a3d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(370, 380)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_37.csv\",\n",
    "    index=False)\n"
   ],
   "id": "661a7cbe631ac56c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(380, 390)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_38.csv\",\n",
    "    index=False)\n"
   ],
   "id": "d32df20a5094e62f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have 300 sites\n",
    "site_ids = range(390, 398)  # Adjust site IDs as needed\n",
    "wdfs = {site_id: prepare_weather(f\"ClimateData/site_{site_id}.txt\") for site_id in\n",
    "        site_ids}  # Prepare wdfs for all sites\n",
    "\n",
    "\n",
    "# Function to process each site and irrigation level\n",
    "def process_site_irr_max(site_id, max_irr, wdf):\n",
    "    smts = optimize(4, max_irr, wdf)  # Replace with your optimization function\n",
    "    yld, tirr, _ = evaluate(smts, max_irr, wdf, True)  # Replace with your evaluation function\n",
    "\n",
    "    return {\n",
    "        'Site_ID': site_id,\n",
    "        'Max_Irrigation_mm': max_irr,\n",
    "        'Optimal_SMTs': smts.tolist(),\n",
    "        'Yield_tonne_per_ha': yld,\n",
    "        'Total_Irrigation_mm': tirr\n",
    "    }\n",
    "\n",
    "\n",
    "# Instead of creating a new list every time, we can use a preallocated list\n",
    "results_list = []\n",
    "\n",
    "# Parallel processing using joblib\n",
    "all_results = Parallel(n_jobs=15)(\n",
    "    delayed(process_site_irr_max)(site_id, max_irr, wdfs[site_id])\n",
    "    for max_irr in max_irr_values\n",
    "    for site_id in site_ids\n",
    ")\n",
    "\n",
    "# Convert the list of results to a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_39.csv\",\n",
    "    index=False)\n"
   ],
   "id": "f9389bd781664852",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the two CSV files\n",
    "df_1 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_1.csv\")\n",
    "df_2 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_2.csv\")\n",
    "df_3 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_3.csv\")\n",
    "df_4 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_4.csv\")\n",
    "df_5 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_5.csv\")\n",
    "df_6 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_6.csv\")\n",
    "df_7 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_7.csv\")\n",
    "df_8 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_8.csv\")\n",
    "df_9 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_9.csv\")\n",
    "df_10 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_10.csv\")\n",
    "df_11 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_11.csv\")\n",
    "df_12 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_12.csv\")\n",
    "df_13 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_13.csv\")\n",
    "df_14 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_14.csv\")\n",
    "df_15 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_15.csv\")\n",
    "df_16 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_16.csv\")\n",
    "df_17 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_17.csv\")\n",
    "df_18 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_18.csv\")\n",
    "df_19 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_19.csv\")\n",
    "df_20 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_20.csv\")\n",
    "df_21 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_21.csv\")\n",
    "df_22 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_22.csv\")\n",
    "df_23 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_23.csv\")\n",
    "df_24 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_24.csv\")\n",
    "df_25 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_25.csv\")\n",
    "df_26 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_26.csv\")\n",
    "df_27 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_27.csv\")\n",
    "df_28 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_28.csv\")\n",
    "df_29 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_29.csv\")\n",
    "df_30 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_30.csv\")\n",
    "df_31 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_31.csv\")\n",
    "df_32 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_32.csv\")\n",
    "df_33 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_33.csv\")\n",
    "df_34 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_34.csv\")\n",
    "df_35 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_35.csv\")\n",
    "df_36 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_36.csv\")\n",
    "df_37 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_37.csv\")\n",
    "df_38 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_38.csv\")\n",
    "df_39 = pd.read_csv(\n",
    "    \"/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/TempDirectory/merged_results_39.csv\")\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_results = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9,\n",
    "                            df_10, df_11, df_12, df_13, df_14, df_15, df_16, df_17, df_18, df_19,\n",
    "                            df_20, df_21, df_22, df_23, df_24, df_25, df_26, df_27, df_28, df_29,\n",
    "                            df_30, df_31, df_32, df_33, df_34, df_35, df_36, df_37, df_38, df_39], ignore_index=True)\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "merged_results.to_csv(\n",
    "    '/Users/tharakajayalath/Library/CloudStorage/OneDrive-UniversityofSaskatchewan/Chapter II-IrrigationValue/Chapter-II/AquaCropOPSyData/WheatMarginal/merged_simulation_results_wheat_marginal_2018_misssing_irrigation.csv',\n",
    "    index=False)\n"
   ],
   "id": "a279e140a791b9ab",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
